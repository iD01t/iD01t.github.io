#!/usr/bin/env python3
"""
Advanced Book Cover Fetcher for iD01t Productions website.
Fetches book covers from multiple sources: Google Books API, Open Library, and more.
"""

import os
import re
import json
import time
import requests
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, quote
import random

def get_random_user_agent():
    """Get a random user agent to avoid detection."""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    return random.choice(user_agents)

def search_google_books_api(title, author="Guillaume Lessard"):
    """Search Google Books API for book covers."""
    try:
        # Clean the title for search
        search_query = f"{title} {author}".replace(':', '').replace('-', ' ').replace('_', ' ')
        search_query = re.sub(r'\s+', ' ', search_query).strip()
        
        # Google Books API endpoint
        url = "https://www.googleapis.com/books/v1/volumes"
        params = {
            'q': search_query,
            'key': 'AIzaSyBxQqQqQqQqQqQqQqQqQqQqQqQqQqQqQ'  # This is a placeholder key
        }
        
        # Try without API key first (limited but free)
        params = {'q': search_query}
        
        response = requests.get(url, params=params, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if 'items' in data and len(data['items']) > 0:
                book = data['items'][0]
                if 'volumeInfo' in book and 'imageLinks' in book['volumeInfo']:
                    cover_url = book['volumeInfo']['imageLinks'].get('thumbnail', '')
                    if cover_url:
                        # Convert to high resolution
                        cover_url = cover_url.replace('zoom=1', 'zoom=3')
                        return cover_url
        
        return None
        
    except Exception as e:
        print(f"        ‚ö†Ô∏è  Google Books API error: {str(e)[:50]}")
        return None

def search_open_library(title, author="Guillaume Lessard"):
    """Search Open Library for book covers."""
    try:
        # Clean the title for search
        search_query = f"{title} {author}".replace(':', '').replace('-', ' ').replace('_', ' ')
        search_query = re.sub(r'\s+', ' ', search_query).strip()
        
        # Open Library search endpoint
        url = "https://openlibrary.org/search.json"
        params = {'q': search_query}
        
        response = requests.get(url, params=params, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if 'docs' in data and len(data['docs']) > 0:
                book = data['docs'][0]
                if 'cover_i' in book:
                    cover_id = book['cover_i']
                    cover_url = f"https://covers.openlibrary.org/b/id/{cover_id}-L.jpg"
                    return cover_url
        
        return None
        
    except Exception as e:
        print(f"        ‚ö†Ô∏è  Open Library error: {str(e)[:50]}")
        return None

def search_google_images(title, author="Guillaume Lessard"):
    """Search Google Images for book covers."""
    try:
        # Clean the title for search
        search_query = f"{title} {author} book cover".replace(':', '').replace('-', ' ').replace('_', ' ')
        search_query = re.sub(r'\s+', ' ', search_query).strip()
        
        # Google Images search
        url = "https://www.google.com/search"
        params = {
            'q': search_query,
            'tbm': 'isch',  # Image search
            'tbs': 'isz:l'  # Large images
        }
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = requests.get(url, params=params, headers=headers, timeout=15)
        
        if response.status_code == 200:
            # Extract image URLs from the response
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for image URLs in the page
            img_tags = soup.find_all('img')
            for img in img_tags:
                src = img.get('src', '')
                if src and src.startswith('http') and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    # Check if it looks like a book cover
                    if any(keyword in src.lower() for keyword in ['cover', 'book', 'title']):
                        return src
            
            # If no specific cover found, return the first reasonable image
            for img in img_tags:
                src = img.get('src', '')
                if src and src.startswith('http') and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    return src
        
        return None
        
    except Exception as e:
        print(f"        ‚ö†Ô∏è  Google Images error: {str(e)[:50]}")
        return None

def search_bing_images(title, author="Guillaume Lessard"):
    """Search Bing Images for book covers."""
    try:
        # Clean the title for search
        search_query = f"{title} {author} book cover".replace(':', '').replace('-', ' ').replace('_', ' ')
        search_query = re.sub(r'\s+', ' ', search_query).strip()
        
        # Bing Images search
        url = "https://www.bing.com/images/search"
        params = {'q': search_query}
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = requests.get(url, params=params, headers=headers, timeout=15)
        
        if response.status_code == 200:
            # Extract image URLs from the response
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for image URLs in the page
            img_tags = soup.find_all('img')
            for img in img_tags:
                src = img.get('src', '')
                if src and src.startswith('http') and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    # Check if it looks like a book cover
                    if any(keyword in src.lower() for keyword in ['cover', 'book', 'title']):
                        return src
            
            # If no specific cover found, return the first reasonable image
            for img in img_tags:
                src = img.get('src', '')
                if src and src.startswith('http') and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    return src
        
        return None
        
    except Exception as e:
        print(f"        ‚ö†Ô∏è  Bing Images error: {str(e)[:50]}")
        return None

def fetch_book_cover_advanced(title, author="Guillaume Lessard"):
    """Fetch book cover using multiple sources."""
    print(f"        üîç Searching for: '{title}' by {author}")
    
    # Try multiple sources in order of preference
    sources = [
        ("Google Books API", lambda: search_google_books_api(title, author)),
        ("Open Library", lambda: search_open_library(title, author)),
        ("Google Images", lambda: search_google_images(title, author)),
        ("Bing Images", lambda: search_bing_images(title, author))
    ]
    
    for source_name, search_func in sources:
        try:
            print(f"        üîç Trying {source_name}...")
            cover_url = search_func()
            
            if cover_url:
                print(f"        ‚úÖ Found cover via {source_name}: {cover_url}")
                return cover_url
            else:
                print(f"        ‚ùå No cover found via {source_name}")
                
        except Exception as e:
            print(f"        ‚ö†Ô∏è  {source_name} error: {str(e)[:50]}")
            continue
        
        # Add delay between sources
        time.sleep(1)
    
    print(f"        ‚ùå No cover found from any source")
    return None

def extract_book_info_from_html(html_content):
    """Extract book title and author from HTML content."""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Try to find title
    title = None
    
    # Method 1: Look for title tag
    title_tag = soup.find('title')
    if title_tag:
        title_text = title_tag.get_text()
        # Clean up the title
        title = title_text.replace(' ¬∑ iD01t Productions', '').replace(' ‚Ä¢ iD01t Productions', '').strip()
    
    # Method 2: Look for h1 tag
    if not title:
        h1_tag = soup.find('h1')
        if h1_tag:
            title = h1_tag.get_text().strip()
    
    # Method 3: Look for meta description
    if not title:
        meta_desc = soup.find('meta', {'name': 'description'})
        if meta_desc:
            desc = meta_desc.get('content', '')
            # Try to extract title from description
            if desc and len(desc) > 10:
                title = desc[:50].strip()
    
    # Default author
    author = "Guillaume Lessard"
    
    return title, author

def update_html_file_with_advanced_cover(html_file_path):
    """Update a single HTML file to use an advanced cover image."""
    print(f"    üìÑ Processing: {html_file_path.name}")
    
    try:
        with open(html_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extract book information
        title, author = extract_book_info_from_html(content)
        
        if not title:
            print(f"        ‚ö†Ô∏è  Could not extract book title")
            return False
        
        print(f"        üìñ Book: '{title}' by {author}")
        
        # Fetch the cover image using advanced methods
        cover_url = fetch_book_cover_advanced(title, author)
        
        if not cover_url:
            print(f"        ‚ùå Failed to fetch cover image from any source")
            return False
        
        print(f"        ‚úÖ Successfully fetched cover: {cover_url}")
        
        # Update the HTML content
        soup = BeautifulSoup(content, 'html.parser')
        
        # Update the main image
        img = soup.find('img')
        if img:
            old_src = img.get('src', '')
            img['src'] = cover_url
            print(f"        üîÑ Updated image from {old_src} to {cover_url}")
        
        # Update Open Graph image
        og_img = soup.find('meta', {'property': 'og:image'})
        if og_img:
            old_og = og_img.get('content', '')
            og_img['content'] = cover_url
            print(f"        üîÑ Updated og:image from {old_og} to {cover_url}")
        
        # Update schema markup
        schema_script = soup.find('script', {'type': 'application/ld+json'})
        if schema_script:
            try:
                schema_data = json.loads(schema_script.string)
                if 'image' in schema_data:
                    old_schema = schema_data['image']
                    schema_data['image'] = cover_url
                    schema_script.string = json.dumps(schema_data, ensure_ascii=False)
                    print(f"        üîÑ Updated schema image from {old_schema} to {cover_url}")
            except Exception as e:
                print(f"        ‚ö†Ô∏è  Could not update schema markup: {e}")
        
        # Save the updated HTML
        with open(html_file_path, 'w', encoding='utf-8') as f:
            f.write(str(soup))
        
        print(f"        üíæ Saved updated HTML file")
        return True
        
    except Exception as e:
        print(f"        ‚ùå Error processing {html_file_path.name}: {e}")
        return False

def update_all_html_files_with_advanced_covers(ebooks_dir, audiobooks_dir):
    """Update all HTML files to use advanced cover images."""
    print("üåê Updating HTML files with advanced cover images...")
    
    updated_count = 0
    total_files = 0
    
    # Process ebooks directory
    print("üìö Processing ebooks directory...")
    for html_file in Path(ebooks_dir).glob('*.html'):
        total_files += 1
        if update_html_file_with_advanced_cover(html_file):
            updated_count += 1
        
        # Add a delay between files to be respectful
        time.sleep(2)
    
    # Process audiobooks directory
    print("üéß Processing audiobooks directory...")
    for html_file in Path(audiobooks_dir).glob('*.html'):
        total_files += 1
        if update_html_file_with_advanced_cover(html_file):
            updated_count += 1
        
        # Add a delay between files to be respectful
        time.sleep(2)
    
    print(f"üìä Update Summary:")
    print(f"   Total files processed: {total_files}")
    print(f"   Successfully updated: {updated_count}")
    print(f"   Failed updates: {total_files - updated_count}")
    
    return updated_count

def test_single_book(title, author="Guillaume Lessard"):
    """Test a single book to see if we can find a cover."""
    print(f"üß™ Testing book: '{title}' by {author}")
    cover_url = fetch_book_cover_advanced(title, author)
    if cover_url:
        print(f"‚úÖ Success: {cover_url}")
    else:
        print(f"‚ùå Failed to fetch cover")
    return cover_url

def main():
    """Main function to update all ebooks with advanced cover images."""
    import argparse
    parser = argparse.ArgumentParser(description='Update ebooks with advanced cover images')
    parser.add_argument('--dry-run', action='store_true', help='Show what would be done without making changes')
    parser.add_argument('--test-book', type=str, help='Test a specific book title')
    parser.add_argument('--repo-path', default='.', help='Path to repository root')
    args = parser.parse_args()
    
    repo_path = Path(args.repo_path)
    ebooks_dir = repo_path / 'ebooks'
    audiobooks_dir = repo_path / 'audiobooks'
    
    if not ebooks_dir.exists():
        print(f"‚ùå Ebooks directory not found: {ebooks_dir}")
        return
    
    if not audiobooks_dir.exists():
        print(f"‚ùå Audiobooks directory not found: {audiobooks_dir}")
        return
    
    print("üöÄ Advanced Book Cover Fetcher for iD01t Productions")
    print("=" * 70)
    
    # Test mode
    if args.test_book:
        test_single_book(args.test_book)
        return
    
    if args.dry_run:
        print("üîç DRY RUN MODE - No changes will be made")
        print()
        
        # Count files and show what would be updated
        ebook_files = list(ebooks_dir.glob('*.html'))
        audiobook_files = list(audiobooks_dir.glob('*.html'))
        
        print(f"üìö Ebooks directory: {len(ebook_files)} HTML files")
        print(f"üéß Audiobooks directory: {len(audiobook_files)} HTML files")
        print(f"üìä Total: {len(ebook_files) + len(audiobook_files)} files")
        
        # Show a few examples of what would be processed
        print("\nüîç Sample files that would be processed:")
        for i, file_path in enumerate(ebook_files[:3]):
            print(f"    üìÑ {file_path.name}")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                title, author = extract_book_info_from_html(content)
                if title:
                    print(f"        üìñ Book: '{title}' by {author}")
                else:
                    print(f"        ‚ö†Ô∏è  Could not extract book info")
            except:
                print(f"        ‚ùå Could not read file")
        
        if len(ebook_files) > 3:
            print(f"    ... and {len(ebook_files) - 3} more ebook files")
        
        print(f"\nüîç DRY RUN COMPLETE")
        print(f"   Would process: {len(ebook_files) + len(audiobook_files)} HTML files")
        print(f"   Would attempt to fetch covers from multiple sources")
        print(f"   Run without --dry-run to actually update the files")
        print(f"   Use --test-book \"Book Title\" to test a specific book")
        
    else:
        print("üîÑ Starting update process...")
        print("‚ö†Ô∏è  This will take some time due to rate limiting...")
        print()
        
        # Update HTML files
        updated_count = update_all_html_files_with_advanced_covers(ebooks_dir, audiobooks_dir)
        print()
        
        print("‚úÖ UPDATE COMPLETE")
        print(f"   All ebooks now use real cover images from multiple sources!")
        print(f"   Updated {updated_count} files successfully")

if __name__ == '__main__':
    main()
